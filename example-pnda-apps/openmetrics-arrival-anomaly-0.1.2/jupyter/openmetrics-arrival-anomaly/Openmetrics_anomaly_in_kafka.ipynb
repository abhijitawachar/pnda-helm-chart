{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('properties.json', 'r') as f:\n",
    "    properties = json.load(f)\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = '{}-cp-kafka:9092'.format(properties['pnda-helm-release'])\n",
    "topic = properties['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_openmetrics(input: str) -> dict:\n",
    "    tokenized = input.split()\n",
    "    source = tokenized[0]\n",
    "    value = float(tokenized[1])\n",
    "    if len(tokenized)>2:\n",
    "        timestamp = float(tokenized[2])\n",
    "    else:\n",
    "        timestamp = None\n",
    "    source_tokenized = source.split(\"{\")\n",
    "    metricname = source_tokenized[0]\n",
    "    if len(source_tokenized)>1:\n",
    "        tags = source_tokenized[1].split(\"}\")[0].split(\",\")\n",
    "    else:\n",
    "        tags = []\n",
    "    return {'source':source,'metricname': metricname,'tags': tags, 'timestamp':timestamp, 'value':value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterArrivalTransformer:\n",
    "    def __init__(self):\n",
    "        self.latest_arrivaltime_dict = dict()\n",
    "\n",
    "    # NOTE: this is implemented for streaming, so it transform one event at a time,\n",
    "    # i.e, the function input is a pandas dataframe with a single row. If multiple rows are provided\n",
    "    # they are iterated with iterrows(). It should be optimized if the function transform full dataframes. \n",
    "    def transform(self, input: pd.DataFrame) -> pd.DataFrame:\n",
    "        interarrival_rows = []\n",
    "        for index,row in input.iterrows():\n",
    "          \n",
    "            source = row['source']\n",
    "            timestamp = row['timestamp']\n",
    "            latest_timestamp = self.latest_arrivaltime_dict.get(source, None)\n",
    "            self.latest_arrivaltime_dict[source] = timestamp\n",
    "            if latest_timestamp:\n",
    "                interarrival_rows.append([source, timestamp, (timestamp - latest_timestamp)])   \n",
    "        return pd.DataFrame(interarrival_rows, \n",
    "                 columns= ['source','timestamp','value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading anomaly detection model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "normal_quantiles_zp = {\n",
    "    0.8: 1.281551565545,\n",
    "    0.9: 1.644853626951,\n",
    "    0.95: 1.959963984540,\n",
    "    0.98: 2.326347874041,\n",
    "    0.99: 2.575829303549,\n",
    "    0.995: 2.807033768344,\n",
    "    0.998: 3.090232306168,\n",
    "    0.999: 3.290526731492,\n",
    "    0.9999: 3.890591886413,\n",
    "    0.99999: 4.417173413469,\n",
    "    0.999999: 4.891638475699,\n",
    "    0.9999999: 5.326723886384,\n",
    "    0.99999999: 5.730728868236,\n",
    "    0.999999999: 6.109410204869,\n",
    "}\n",
    "\n",
    "# Define the model class\n",
    "#class MultisourceNormalDetector(mlflow.pyfunc.PythonModel):\n",
    "class MultisourceNormalDetector:\n",
    "    def __init__(self):\n",
    "        self.stats = {}\n",
    "\n",
    "    \n",
    "    def fit(self, input: pd.DataFrame):\n",
    "        # Non-incremental avg-var computation. Stats are reset\n",
    "        self.reset()\n",
    "        agg_df = input.groupby(['source'])['value'].agg(['sum','count','var'])\n",
    "        for source,row in agg_df.iterrows():\n",
    "            stats = {'sum': row['sum'],\n",
    "                        'count': row['count'],\n",
    "                        'avg': row['sum']/row['count'],\n",
    "                        'var': row['var']}\n",
    "            self.stats[source] = stats\n",
    "\n",
    "    def incremental_fit(self, input: pd.DataFrame):\n",
    "        # incremental avg-var computation\n",
    "        # https://math.stackexchange.com/questions/102978/incremental-computation-of-standard-deviation\n",
    "        for index,row in input.iterrows():\n",
    "            stats = self.stats.get(row['source'],{'sum':0,'count':0, 'var':0})\n",
    "            sum = stats['sum'] + row['value']\n",
    "            count = stats['count'] + 1\n",
    "            if count > 1:\n",
    "                stats['var'] = (count-2)/(count-1) * stats['var'] + 1/count * math.pow((row['value'] - stats['avg']),2)\n",
    "            else:\n",
    "                stats['var'] = stats['var'] + math.pow(row['value']- sum/count,2)\n",
    "            stats['avg'] = sum/count\n",
    "            stats['sum'] = sum\n",
    "            stats['count'] = count\n",
    "            self.stats[row['source']] = stats\n",
    "\n",
    "    def reset(self,source=None):\n",
    "        if source:\n",
    "            self.stats[source] = {}\n",
    "        else:\n",
    "            self.stats = {}\n",
    "\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame, p=0.99999) -> pd.DataFrame:\n",
    "        #NOTE can be optimized using pandas to perform this condition check\n",
    "        output = []\n",
    "        for index,row in model_input.iterrows():  \n",
    "            source = row['source']\n",
    "            value = row['value']\n",
    "            timestamp = row['timestamp']\n",
    "            if source not in self.stats:\n",
    "                output_row = [source, str(timestamp), value, True, 'unknown source']\n",
    "            else:\n",
    "                stats = self.stats[source]\n",
    "                zp = normal_quantiles_zp[p]\n",
    "                diff = zp*math.sqrt(stats['var'])\n",
    "                min = stats['avg'] - diff\n",
    "                max = stats['avg'] + diff\n",
    "                if min <= value <= max:\n",
    "                    output_row = [source, str(timestamp), value, False, None]\n",
    "                else:\n",
    "                    output_row = [source, str(timestamp), value, True, 'value out of limits ({},{})'.format(min,max)]\n",
    "            output.append(output_row)\n",
    "        return pd.DataFrame(output, \n",
    "                 columns= ['source','timestamp','value','anomaly','anomaly_type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultisourceNormalDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "def run_fit(brokers, topic, model, interarrival_transformer):\n",
    "    try:\n",
    "        consumer = KafkaConsumer(topic, bootstrap_servers=brokers.split())    \n",
    "        for msg in consumer:\n",
    "            openmetric_sample=decode_openmetrics(msg.value.decode('utf-8'))\n",
    "            if openmetric_sample['timestamp'] == None:\n",
    "                openmetric_sample['timestamp'] = msg.timestamp\n",
    "            openmetric_df = pd.DataFrame.from_dict(openmetric_sample)\n",
    "            interrarival_df = interarrival_transformer.transform(openmetric_df)\n",
    "            model.incremental_fit(interrarival_df)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Model Fitting Stage interrupted.\")\n",
    "        print(\"model:       avg           var        count  source\")\n",
    "        for k,v in model.stats.items():\n",
    "            print(\"      {:10.2f}    {:10.2f} {:10.2f}  {}\".format(v['avg'],v['var'],v['count'], k ))\n",
    "\n",
    "fit_interarrival_transformer = InterArrivalTransformer()\n",
    "run_fit(brokers, topic, model, fit_interarrival_transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting anomalies from source\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Predict Model\n",
    "def run_predict( brokers, topic, model, interarrival_transformer):\n",
    "    consumer = KafkaConsumer(topic, bootstrap_servers=brokers.split())    \n",
    "    for msg in consumer:\n",
    "        openmetric_sample=decode_openmetrics(msg.value.decode('utf-8'))\n",
    "        if openmetric_sample['timestamp'] == None:\n",
    "            openmetric_sample['timestamp'] = msg.timestamp\n",
    "        openmetric_df = pd.DataFrame.from_dict(openmetric_sample)\n",
    "        anomalies_df = model.predict(context=[], model_input=interarrival_transformer.transform(openmetric_df), p=0.999999)\n",
    "        if len(anomalies_df[anomalies_df.anomaly == True]) > 0:\n",
    "            print(anomalies_df[anomalies_df.anomaly == True])\n",
    "\n",
    "predict_interarrival_transformer = InterArrivalTransformer()\n",
    "run_predict(brokers, topic, model, predict_interarrival_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
